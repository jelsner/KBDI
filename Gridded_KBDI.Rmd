---
title: "KBDI using PRISM temperature and precipitation"
output: html_document
editor_options: 
  chunk_output_type: console
---

Compute KBDI using PRISM temp/precip data.
1. Read PRISM .bil files as {stars} objects (see `stars.Rmd`)
2. Filter on the ANF boundary
3. Loop over all grid points (e.g., tp.stars["tmax", 1, 1, ] is the time series of the high temperature for grid location 1, 1).
4. Save computed Qlm as a variable in the `tp.stars` object.
5. Map correlations between gridded Qlm and TLH_Qlm 
6. Use Jakub Nowosad @jakub_nowosad May 21 to create super-pixels of the correlation.

```{r}
library(stars)
library(tidyverse)
library(lubridate)
library(rgdal) # before library(sf)
library(sf)
library(here)
library(prism)
library(ggthemes)
```

Start with one year. 2019.

## PRISM data

Download daily PRISM data using functions from the {prism} package. About 35 minutes per year per variable of daily data at home internet speeds. Temperatures are in C and precipitation is in mm.
```{r}
prism_set_dl_dir("Data/PRISM/")

t0 <- proc.time()
get_prism_dailys(type = "ppt",
                 minDate = "2017-01-01",
                 maxDate = "2017-12-31")
proc.time() - t0
```

Create a {stars} object using high temperature (`tmax`) and daily rainfall (`ppt`).
```{r}
t0 <- 2017
t1 <- 2019
dates <- seq(as_date(paste0(t0, "-01-01")), 
             as_date(paste0(t1, "-12-31")), 
             by = "day")

file_days <- gsub("-", "", dates)

folder_names <- paste0("PRISM_tmax_stable_4kmD2_", file_days, "_bil/")
file_names <- paste0("PRISM_tmax_stable_4kmD2_", file_days, "_bil.bil")
file_list <- paste0("Data/", "PRISM/", folder_names, file_names)

folder_names2 <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil/")
file_names2 <- paste0("PRISM_ppt_stable_4kmD2_", file_days, "_bil.bil")
file_list2 <- paste0("Data/", "PRISM/", folder_names2, file_names2)

tmax <- read_stars(file_list, along = list(time = dates)) %>%
  setNames("Tmax_C")
ppt <- read_stars(file_list2, along = list(time = dates)) %>%
  setNames("Precip_mm")
TP.st <- c(tmax, ppt)

TP.st
```

Crop to ANF boundary
```{r}
ANF_Boundary.sf <- st_read(dsn = "S_USA.NFSLandUnit") %>%
  filter(NFSLANDU_2 == "Apalachicola National Forest") %>%
  st_transform(crs = st_crs(TP.st))

ANF_BBox.sfc <- 
  ANF_Boundary.sf %>%
  st_bbox() %>%
  st_as_sfc()

TP.st <- 
  TP.st %>%
  st_crop(ANF_BBox.sfc,
          crop = TRUE)
```

Check with plot
```{r}
X <- filter(TP.st, time >= ymd("2019-01-01"), time <= ymd("2019-01-31"))

ggplot() +  
  geom_stars(data = X[1], alpha = .8, downsample = c(1, 1, 1)) + 
  facet_wrap("time") +
  scale_fill_distiller(palette = "RdBu") +
  coord_equal() +
  theme_map() +
  theme(legend.position = "bottom") +
  theme(legend.key.width = unit(2, "cm"))
```

Compute KBDI at each grid location. Start by converting temperatures in C to F and precipitation in mm to inches
```{r}
TP.st <- 
  TP.st %>%
  mutate(MaxTemp = Tmax_C * 1.8 + 32,
         MaxTemp = replace_na(MaxTemp, mean(MaxTemp)),
         Rainfall24 = Precip_mm * .03937,
         Rainfall24 = replace_na(Rainfall24, 0))
```

Compute net rainfall on each at each grid point.
```{r}
NetR.a <- array(NA, dim = dim(TP.st)) # initialize the 3-D array with NAs

for(i in 1:dim(TP.st)[1]){
  for(j in 1:dim(TP.st)[2]){
Rainfall24 <- TP.st$Rainfall24[i, j, ]
PR <- dplyr::lag(Rainfall24)
PR[1] <- 0

CumR <- 0
NetR <- numeric()

for(t in 1:length(Rainfall24)) {
  R24 <- Rainfall24[t]
  if (R24 == 0) {
    NetR[t] <- 0
    CumR <- 0
  } 
  else if(R24 > 0 & R24 <= .2) {
      CumR <- CumR + R24
      if (PR[t] > .2 | CumR > .2) NetR[t] <- R24
      else if (CumR > .2) NetR[t] <- CumR - .2
      else NetR[t] <- 0
    }
  else if (R24 > .2) {
      if (CumR <= .2) {
      NetR[t] <- CumR + R24 - .2
      CumR <- CumR + R24
      }
      else {
      NetR[t] <- R24
      CumR <- CumR + R24
      }
  }
}

NetR.a[i, j, ] <- NetR
}
}

TP.st$NetR <- NetR.a
```

Compute daily drought index.
```{r}
R <- 59.23 # average annual rainfall for TLH in inches

Ql.a <- array(NA, dim = dim(TP.st)) # initialize the 3-D array with NAs

for(i in 1:dim(TP.st)[1]){
  for(j in 1:dim(TP.st)[2]){
Q <- 269  
NetR <- TP.st$NetR[i, j,  ]
MaxTemp <- TP.st$MaxTemp[i, j, ]

Ql <- numeric()
DeltaQl <- numeric()
for(t in 1:length(NetR)){
  DeltaQ <- (800 - Q) * (.968 * exp(.0486 * MaxTemp[t]) - 8.3) /(1 + 10.88 * exp(-.0441 * R)) * .001 
  Q <- ifelse(NetR[t] == 0,  Q + DeltaQ,  (Q + DeltaQ) - NetR[t] * 100)
  Q <- ifelse(Q < 0, 0, Q) 
  Ql <- c(Ql, Q)
  DeltaQl <- c(DeltaQl, DeltaQ)
}

Ql.a[i, j, ] <- Ql
}
}

TP.st$Ql <- Ql.a
TP.st$Qlm <- TP.st$Ql * .254 # tenth of an inch to mm
TP.st$DroughtIndex <- floor(TP.st$Ql/100)
```

Check with plot
```{r}
X <- filter(TP.st, time >= ymd("2019-10-01"), time <= ymd("2019-10-31"))

ggplot() +  
  geom_stars(data = X[7], alpha = .8, downsample = c(1, 1, 1)) + 
  facet_wrap("time") +
  scale_fill_distiller(palette = "RdBu") +
  coord_equal() +
  theme_map() +
  theme(legend.position = "bottom") +
  theme(legend.key.width = unit(2, "cm"))
```

Compute Qlm using the TLH data. 
```{r}
TLH.df <- read.csv(file = 'Data/TLH_Daily1940.csv',
                   stringsAsFactors = FALSE,
                   header = TRUE) %>%
  mutate(Date = as.Date(DATE)) %>%
  mutate(Year = year(Date), 
         month = month(Date, label = TRUE, abbr = TRUE),
         doy = yday(Date),
         MaxTemp = TMAX,
         MinTemp = TMIN,
         Rainfall24 = PRCP,
         Rainfall24 = replace_na(Rainfall24, 0),
         Rainfall24mm = Rainfall24 * 25.4) %>%
  filter(Year >= 2017 & Year <= 2019)

Rainfall24 <- TLH.df$Rainfall24
PR <- dplyr::lag(Rainfall24)
PR[1] <- 0

CumR <- 0
NetR <- numeric()

for(i in 1:length(Rainfall24)) {
  R24 <- Rainfall24[i]
  if (R24 == 0) {
    NetR[i] <- 0
    CumR <- 0
  } 
  else if(R24 > 0 & R24 <= .2) {
      CumR <- CumR + R24
      if (PR[i] > .2 | CumR > .2) NetR[i] <- R24
      else if (CumR > .2) NetR[i] <- CumR - .2
      else NetR[i] <- 0
    }
  else if (R24 > .2) {
      if (CumR <= .2) {
      NetR[i] <- CumR + R24 - .2
      CumR <- CumR + R24
      }
      else {
      NetR[i] <- R24
      CumR <- CumR + R24
      }
  }
}

TLH.df$NetR <- NetR

Q <- 269  
R <- 59.23 # average annual rainfall for TLH in inches

MaxTemp <- TLH.df$MaxTemp

Ql <- numeric()
DeltaQl <- numeric()
for(i in 1:length(Rainfall24)){
  DeltaQ <- (800 - Q) * (.968 * exp(.0486 * MaxTemp[i]) - 8.3) /(1 + 10.88 * exp(-.0441 * R)) * .001 
  Q <- ifelse(NetR[i] == 0,  Q + DeltaQ,  (Q + DeltaQ) - NetR[i] * 100)
  Q <- ifelse(Q < 0, 0, Q) 
  Ql <- c(Ql, Q)
  DeltaQl <- c(DeltaQl, DeltaQ)
}

TLH.df$Ql <- Ql
TLH.df$Qlm <- Ql * .254  # tenth of an inch to mm
TLH.df$DeltaQl <- DeltaQl
TLH.df$DroughtIndex <- floor(Ql/100)
```

Loop over all grid locations computing the correlation between the Qlm computed using TLH data and the Qlm's computed using PRISM data.
```{r}
R.a <- array(NA, dim = dim(TP.st)[1:2])

for(i in 1:dim(TP.st)[1]){
  for(j in 1:dim(TP.st)[2]){
    
Qlm <- TP.st$Qlm[i, j, ]

cc <- cor(Qlm, TLH.df$Qlm)

R.a[i, j] <- cc
}
}

R.st <- st_as_stars(R.a)

attr(R.st, "dimensions")$y$delta = -1

ggplot() +  
  geom_stars(data = R.st[1], alpha = .8, downsample = c(1, 1, 1)) + 
  scale_fill_distiller(limits = c(0, 1), palette = "Blues", direction = 1) +
  coord_equal() +
  theme_map() +
  theme(legend.position = "bottom") +
  theme(legend.key.width = unit(2, "cm"))
```

Redimension the correlation data and create a proper map.


